\section{Добыча корпуса документов}

В рамках выполнения лабораторных работ был сформирован специализированный корпус текстовых документов, посвящённый тематике футбола. Корпус предназначен для использования на всех последующих этапах задач информационного поиска.

\subsection*{Источники данных}

Для формирования корпуса были использованы материалы из следующих независимых источников:

\begin{itemize}
    \item \textbf{Википедия}— энциклопедические статьи, относящиеся к футболу (клубы, соревнования, футболисты, тренеры, стадионы и т.д.);
    \item \textbf{Чемпионат} — аналитические и публицистические материалы футбольной тематики;
    \item \textbf{Спортс} — длинные авторские статьи и обзоры, посвящённые футбольным событиям.
\end{itemize}

Использование разнородных источников позволило сформировать корпус, сочетающий как энциклопедический, так и журналистский стили текста.

\subsection*{Характеристика исходных документов}

Исходные документы были получены в формате HTML и содержали, помимо основного текста, значительный объём дополнительной информации:

\begin{itemize}
    \item HTML-разметку (теги, вложенные контейнеры);
    \item навигационные элементы (меню, ссылки, блоки рекомендаций);
    \item метаданные (заголовки, даты публикации, идентификаторы страниц);
    \item вспомогательный контент (скрипты, стили).
\end{itemize}

Для Википедии дополнительно использовалась структурированная категорияльная разметка, позволяющая организовать обход статей по тематическим категориям с ограничением глубины. Для новостных сайтов применялся постраничный обход списков публикаций с фильтрацией по типу URL.

\subsection*{Выделение текстового содержимого}

Из HTML-документов был выделен чистый текст с использованием эвристик, ориентированных на семантически значимые блоки страницы. В качестве источников текста использовались элементы \texttt{<article>}, \texttt{<main>}, контейнеры с типичными классами контента, а также текстовые абзацы и заголовки.

При обработке документов выполнялись следующие операции:
\begin{itemize}
    \item удаление HTML-тегов и служебных элементов;
    \item нормализация пробельных символов;
    \item устранение дублирующихся фрагментов текста;
    \item фильтрация слишком коротких документов.
\end{itemize}

Для обеспечения качества корпуса вводилось минимальное ограничение на длину текста в словах, что позволило исключить краткие заметки и страницы с недостаточным содержанием.

\subsection*{Возможность использования внешних поисковых систем}

Для всех выбранных источников существуют действующие поисковые механизмы, что удовлетворяет требованиям ТЗ:

\begin{itemize}
    \item встроенный поиск Википедии по статьям;
    \item поиск Google с ограничением по домену (например, \texttt{site:ru.wikipedia.org});
    \item внутренний поиск по сайтам Sports.ru и Championat.com.
\end{itemize}

Примеры запросов:
\begin{itemize}
    \item \texttt{site:ru.wikipedia.org Лионель Месси};
    \item \texttt{site:championat.com Лига чемпионов финал};
    \item \texttt{site:sports.ru трансферы футбол}.
\end{itemize}

Основные недостатки существующих поисковых систем:
\begin{itemize}
    \item отсутствие прозрачного ранжирования;
    \item невозможность гибкой обработки словоформ;
    \item смешение текстового контента с мультимедийными и навигационными элементами;
    \item ограниченные возможности анализа и воспроизводимости результатов.
\end{itemize}

Данные ограничения дополнительно показывают целесообразность построения собственного поискового индекса.

\subsection*{Статистическая информация о корпусе}

В результате выполнения сбора и фильтрации данных был сформирован корпус, содержащий \textbf{34\,583 документа}.

Основные характеристики корпуса представлены ниже:
\begin{itemize}
    \item количество документов: 34\,583;
    \item суммарный объём «сырых» HTML-документов: около 24\,114\,053\,120 байт;
    \item суммарный объём выделенного текстового содержимого: около 1\,916\,807\,680 байт;
    \item средний размер одного исходного HTML-документа: 755\,170 байт;
    \item средний объём очищенного текста одного документа: 68\,628 байт;
    \item среднее количество слов в документе: 1\,357.
\end{itemize}

Корпус характеризуется высокой тематической однородностью, значительным объёмом текста и наличием разнообразных стилей изложения, что делает его пригодным для проведения всех последующих лабораторных работ..

\pagebreak

\section{Поисковый робот}
\label{sec:robot}

Поисковый робот предназначен для автоматизированной обкачки набора веб-документов, извлечения из них текстового содержимого и сохранения результата в базу данных MongoDB. Робот запускается одной командой и принимает единственный аргумент командной строки — путь до YAML-конфигурации:
\begin{verbatim}
python lr2_robot.py config.yaml
\end{verbatim}


\subsection*{Архитектура и общий конвейер}

Архитектура робота построена как конвейер из двух фаз, работающих параллельно:
\begin{enumerate}
  \item \textbf{Discovery (наполнение очереди задач)} — отдельные потоки генерируют URL для обкачки и складывают их в коллекцию \texttt{tasks}.
  \item \textbf{Worker (обкачка и сохранение результата)} — воркеры извлекают задачу из \texttt{tasks}, скачивают документ, парсят текст и пишут результат в \texttt{documents}, после чего планируют следующую переобкачку этой страницы.
\end{enumerate}

Подход с отдельной очередью в MongoDB обеспечивает два ключевых свойства из ТЗ:
\begin{itemize}
  \item \textbf{Возобновление после остановки}: состояние задач хранится в базе; при перезапуске воркеры продолжают с задач, у которых настало время \texttt{next\_fetch\_at} и которые не заблокированы.
  \item \textbf{Переобкачка}: задача после выполнения планируется снова через \texttt{recrawl\_seconds} — это реализует периодическую переобкачку.
\end{itemize}

\subsection*{Нормализация URL}

В роботе реализована функция \texttt{normalize\_url(url)}, которая:
\begin{itemize}
  \item приводит схему и домен к нижнему регистру;
  \item удаляет fragment (\#...);
  \item удаляет/фильтрует трекинговые параметры (например, \texttt{utm\_*}, \texttt{gclid}, \texttt{fbclid} и т.д.);
  \item сортирует query-параметры для канонического представления;
  \item нормализует путь (убирает двойные ``//'', убирает конечный ``/'' в большинстве случаев).
\end{itemize}

Это критично для дедупликации: одна и та же страница может встречаться с разными UTM-метками, а нормализация гарантирует, что в базе будет единая запись.

\subsection*{Хранение в MongoDB}

В MongoDB используются две коллекции.

\subsubsection*{Коллекция \text{documents}}
Содержит итоговые документы. Ключевые поля:
\begin{itemize}
  \item \texttt{source} — название источника (например, \texttt{wiki}, \texttt{championat}, \texttt{sportsru});
  \item \texttt{url\_norm} — нормализованный URL;
  \item \texttt{raw\_html} — ``сырой'' HTML (сохраняется при изменении контента);
  \item \texttt{parsed\_text} — очищенный текст (используется далее в задачах ИП);
  \item \texttt{fetched\_at} — время обкачки в формате Unix timestamp;
  \item \texttt{content\_hash} — SHA-256 хэш очищенного текста, применяемый для детекта изменений;
  \item \texttt{http\_etag}, \texttt{http\_last\_modified} — заголовки для условных запросов;
  \item \texttt{status\_code} — HTTP статус последней обкачки;
  \item \texttt{word\_count} — число слов (контроль качества/фильтрация).
\end{itemize}

\subsubsection*{Коллекция \text{tasks}}
Это очередь задач с сохранением состояния:
\begin{itemize}
  \item \texttt{state} — состояние задачи (\texttt{queued}, \texttt{fetching}, \texttt{done}, \texttt{error});
  \item \texttt{next\_fetch\_at} — Unix timestamp времени, когда задачу можно выполнять снова;
  \item \texttt{locked\_until} и \texttt{locked\_by} — механизм блокировок, чтобы несколько потоков/процессов не взяли одну задачу;
  \item \texttt{retries}, \texttt{last\_error} — учёт ошибок и повторов;
  \item \texttt{priority} — приоритет задачи (например, выше у Wikipedia discovery);
  \item \texttt{meta} — метаданные (например, title у wiki, listing-страница у новостных источников).
\end{itemize}

\subsubsection*{Индексы}
Для производительности и дедупликации создаются индексы:
\begin{itemize}
  \item уникальный \texttt{(source, url\_norm)} в \texttt{documents} и \texttt{tasks} — исключает дубли;
  \item индексы по \texttt{state}, \texttt{next\_fetch\_at}, \texttt{locked\_until} — ускоряют выборку готовых задач;
  \item индекс по \texttt{content\_hash} — ускоряет аналитические операции/поиск по хэшам.
\end{itemize}

\subsection*{Механизм остановки и корректного завершения}

Для безопасной остановки используется глобальное событие \texttt{STOP\_EVENT} и обработчики сигналов \texttt{SIGINT/SIGTERM}. При нажатии Ctrl+C выставляется флаг остановки, потоки прекращают:
\begin{itemize}
  \item discovery перестаёт добавлять новые задачи;
  \item worker-циклы выходят, завершаясь после текущих операций;
  \item функция \texttt{sleep\_with\_stop()} позволяет прерывать ожидания (задержки, backoff) без зависания на долгом sleep.
\end{itemize}

Поскольку состояние очереди хранится в MongoDB, повторный запуск продолжает работу без ручного восстановления.

\subsection*{Переобкачка}

В роботе это реализовано двумя взаимодополняющими способами.

\subsubsection*{1) Условные HTTP-запросы (ETag/Last-Modified) и статус 304}

Если в метаданных задачи есть \texttt{http\_etag} и/или \texttt{http\_last\_modified}, воркер отправляет заголовки:
\begin{itemize}
  \item \texttt{If-None-Match: <etag>}
  \item \texttt{If-Modified-Since: <last-modified>}
\end{itemize}
Если сервер отвечает \texttt{304 Not Modified}, робот:
\begin{itemize}
  \item не сохраняет HTML/текст заново;
  \item фиксирует факт проверки и планирует следующую переобкачку через \texttt{recrawl\_seconds}.
\end{itemize}

\subsubsection*{2) Сравнение хэша очищенного текста}

Если получен новый документ (статус 200), выполняется:
\begin{itemize}
  \item парсинг и извлечение \texttt{parsed\_text};
  \item вычисление \texttt{new\_hash = sha256(parsed\_text)};
  \item получение старого хэша \texttt{old\_hash} из \texttt{documents};
  \item признак \texttt{changed = (old\_hash != new\_hash)}.
\end{itemize}

Если \texttt{changed = true}, тогда обновляются \texttt{raw\_html}, \texttt{parsed\_text}, \texttt{content\_hash}. Если \texttt{changed = false}, обновляются только служебные поля последней проверки (\texttt{fetched\_at}, HTTP заголовки и т.п.), а сам контент не переписывается. Это уменьшает запись в БД и упрощает последующую обработку корпуса.

\subsection*{Регулирование интенсивности запросов и обработка сетевых сбоев}

\subsubsection*{Ограничение частоты запросов}
Задержка \texttt{delay\_seconds} реализована классом \texttt{RateLimiter}. Он хранит момент следующего разрешённого запроса и синхронизируется через mutex. Это гарантирует, что даже при нескольких потоках не будет превышения заданного темпа запросов к источнику.

\subsubsection*{Повторы при ошибках}
HTTP-запросы выполняются через \texttt{requests.Session()} (класс \texttt{Fetcher}). При сетевых исключениях применяется retry с увеличением задержки. На уровне задач дополнительно ведётся счётчик \texttt{retries} и применяется экспоненциальный backoff, что повышает устойчивость к временным проблемам сети/источника и снижает риск постоянной перегрузки сайта.

\subsection*{Генерация задач для разных источников}

Робот поддерживает несколько источников, каждый со своей стратегией discovery.

\subsubsection*{Wikipedia}
Discovery для Wikipedia строится вокруг категорий:
\begin{itemize}
  \item на вход подаются \texttt{seed\_categories} (например, «Категория:Футбол»);
  \item выполняется обход в ширину по подкатегориям до глубины \texttt{max\_depth};
  \item из каждой категории извлекаются страницы (\texttt{cmtype=page});
  \item каждая страница превращается в URL, нормализуется и ставится в очередь задач (\texttt{tasks}).
\end{itemize}
Для скачивания HTML используется API Wikipedia.

\subsubsection*{Championat.com}
Discovery реализован обходом страниц ленты вида:
\texttt{https://www.championat.com/
articles/football/<page>.html}.
Из листинга извлекаются ссылки, отфильтрованные регулярным выражением для статей (страницы вида \texttt{/football/article-<id>...html}). Для защиты от бесконечного перебора предусмотрена эвристика «остановиться после нескольких подряд 404».

\subsubsection*{Sports.ru}
Discovery берёт мобильную версию \texttt{m.sports.ru} и проходит страницы блогов футбола:
\texttt{/football/blogs/} и \texttt{/football/blogs/pageN/}.
Ссылки фильтруются по наличию \texttt{/football/blogs/} в пути.

\subsection*{Парсинг и очистка текста}

Для разных источников используются отдельные функции парсинга:
\begin{itemize}
  \item \texttt{parse\_wiki\_html()} — выбирает контентные блоки Wikipedia, удаляет навигационные таблицы, списки ссылок, TOC, подписи и т.д., затем собирает абзацы/заголовки.
  \item \texttt{parse\_championat\_html()} и \texttt{parse\_sportsru\_html()} — извлекают содержимое из \texttt{article/main}, удаляют служебные теги (\texttt{script/style/header/footer/...}), и собирают связный текст.
\end{itemize}

В качестве контроля качества используется метрика \texttt{word\_count(parsed\_text)}. Если текст слишком короткий (меньше \texttt{min\_words} для источника), документ пропускается (задача помечается выполненной с причиной \texttt{too\_short}). Это позволяет отсекать навигационные страницы, страницы-листинги и пустые материалы.

\subsection*{Потоки и параллелизм}

В программе одновременно работают:
\begin{itemize}
  \item 1--3 потока discovery (по числу включённых источников),
  \item по \texttt{worker\_threads\_per\_source} потоков на каждый источник,
  \item отдельный поток статистики \texttt{progress\_loop}, который периодически выводит агрегированное состояние задач и количество документов.
\end{itemize}

Механизм блокировок задач (поля \texttt{locked\_until}/\texttt{locked\_by}) позволяет безопасно масштабировать воркеры (в том числе потенциально на несколько процессов), не нарушая целостности очереди.

\pagebreak


\section{Токенизация}

Токенизация является первым этапом построения поисковой системы и заключается в разбиении текстового содержимого документов на элементарные единицы~— токены, которые в дальнейшем используются при индексировании и обработке поисковых запросов. Качество токенизации напрямую влияет на полноту и точность поиска, а также на объём и структуру индекса.

\subsection*{Правила токенизации}

В реализованной системе используется детерминированная символьная токенизация со следующими правилами:
\begin{itemize}
    \item текст обрабатывается как последовательность символов Unicode;
    \item токеном считается непрерывная последовательность букв и/или цифр;
    \item все разделители (пробелы, знаки препинания, служебные символы) используются как границы токенов и не включаются в результат;
    \item регистр символов игнорируется (все токены приводятся к нижнему регистру);
    \item токены сохраняются в том виде, в котором они встречаются в тексте, без лемматизации и стемминга.
\end{itemize}

Результатом работы токенизатора является файл с идентификатором документа и выделенным токеном. Пример выходных данных представлен ниже:
\begin{verbatim}
0   История
0   футбола
1   Федерация
1   ФИФА
\end{verbatim}

\subsection*{Достоинства и недостатки выбранного метода}

Основными достоинствами выбранного подхода являются:
\begin{itemize}
    \item простота реализации и высокая скорость работы;
    \item отсутствие зависимостей от внешних библиотек;
    \item универсальность для текстов на разных языках;
    \item линейная сложность по длине входного текста.
\end{itemize}

К недостаткам метода можно отнести:
\begin{itemize}
    \item отсутствие нормализации словоформ (например, \texttt{футбол}, \texttt{футбола}, \texttt{футбольных} считаются разными токенами);
    \item некорректную обработку составных токенов и специальных форматов;
    \item избыточное количество числовых и служебных токенов.
\end{itemize}

\subsection*{Примеры неудачной токенизации}

В процессе анализа корпуса были выявлены токены, которые можно считать неудачными:
\begin{itemize}
    \item \texttt{ISBN}, \texttt{978-5-17-044765-7}~— идентификаторы и номера;
    \item одиночные буквы (\texttt{А}, \texttt{М}, \texttt{К}), появляющиеся из-за сокращений;
    \item языковые пометки (\texttt{англ}, \texttt{исп}).
\end{itemize}

Для улучшения качества токенизации можно:
\begin{itemize}
    \item вводить минимальную длину токена;
    \item удалять или понижать вес числовых токенов;
    \item объединять дефисные конструкции;
    \item применять лемматизацию или стемминг на этапе токенизации либо индексации.
\end{itemize}

\subsection*{Статистические характеристики и производительность}

В результате токенизации корпуса, , были получены следующие статистические характеристики:
\begin{itemize}
    \item общее количество токенов: $25\,738\,456$;
    \item средняя длина токена: $10{,}7$ байт;
    \item среднее количество токенов на документ: $1\,259$.
\end{itemize}

Токенизация всего корпуса заняла около $35{,}9$ секунды; время работы линейно зависит от объёма входных данных ($O(n)$). Средняя скорость токенизации составляет порядка $7{,}2 \cdot 10^{5}$ токенов в секунду, что близко к пределу для однопоточной реализации на CPU. Ускорение возможно за счёт оптимизации ввода-вывода и распараллеливания обработки документов.


\pagebreak


\section{Закон Ципфа}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{zipf.png}
    \caption{Распределение терминов по частотам и аппроксимация законом Ципфа (логарифмические шкалы)}
    \label{fig:zipf}
\end{figure}


Для корпуса документов было построено распределение терминов по убыванию их частот в логарифмических координатах и наложена теоретическая аппроксимация, соответствующая закону Ципфа, согласно которому частота термина обратно пропорциональна его рангу. На большей части диапазона рангов эмпирическая кривая близка к линейной в логарифмической шкале, что подтверждает выполнение закона Ципфа и типичное статистическое поведение терминов в корпусе естественного языка.

Наблюдаемые расхождения с теоретической кривой объясняются особенностями реального корпуса. Для наиболее частотных терминов отклонения связаны с удалением служебных слов и тематической неоднородностью документов, а в области больших рангов — с конечным размером корпуса и дискретностью частот, когда значительная часть терминов встречается один или несколько раз. Такие отклонения являются ожидаемыми и не противоречат применимости закона Ципфа к данному корпусу.


\pagebreak

\section{Стемминг}

В рамках данной работы в реализованную поисковую систему был добавлен механизм стемминга — упрощённой нормализации словоформ, направленной на приведение различных грамматических форм слова к общему основанию (стему). Основной целью внедрения стемминга является повышение полноты поиска за счёт устранения зависимости результатов от конкретной словоформы, используемой в запросе или документе.

\subsection*{Способ внедрения стемминга}

Стемминг был реализован на этапе индексации и выполнения запроса. Для каждого токена исходного текста вычисляется его стем, после чего формируется дополнительный индекс, сопоставляющий стем с множеством документов и частотами вхождения. Параллельно сохраняется информация о точных формах слов, что позволяет учитывать как нормализованные, так и исходные токены.

При выполнении поискового запроса запросные термы также подвергаются стеммингу. Поиск производится по стем-индексу, а документы, содержащие точное совпадение исходной словоформы, получают дополнительный вес (бонус к релевантности). Такой подход позволяет объединить преимущества поиска по нормализованным формам и точного совпадения.

Реализация стемминга выполнена в исходном коде поисковой системы на языке C++. Нормализация токенов осуществляется в функции обработки термов, используемой как на этапе индексирования корпуса, так и при обработке пользовательского запроса. В ходе индексирования для каждого токена вычисляется его стем, после чего обновляется структура стем-индекса, сопоставляющая стем с идентификаторами документов и частотами вхождений.

\subsection*{Оценка качества поиска}

Оценка качества поиска проводилась путём сравнения результатов до и после внедрения стемминга на наборе тестовых запросов. Для каждого запроса анализировались:
\begin{itemize}
    \item состав выдачи;
    \item позиции релевантных документов;
    \item наличие нерелевантных документов в верхней части списка.
\end{itemize}

В большинстве случаев внедрение стемминга привело к улучшению качества поиска. Это выражалось в увеличении числа релевантных документов в выдаче и снижении чувствительности к морфологическим вариациям слов. Особенно заметный положительный эффект наблюдался для запросов, содержащих глаголы и существительные с большим числом словоформ.

Результаты сравнения качества поиска до и после внедрения стемминга сохранялись в виде таблицы, содержащей информацию о запросе и составе выдачи. Фрагмент такого файла приведён ниже:

\begin{verbatim}
query=футбол
before: 12, 45
after:  12, 45, 103, 217

query=игрок
before: 34
after:  21, 34, 89, 190

query=забил
before: 87
after:  54, 87, 142
\end{verbatim}

Для приведённых запросов внедрение стемминга привело к увеличению полноты поиска. В выдачу стали попадать документы, содержащие различные словоформы терминов (например, «игрок», «игроки», «игроков» или «забил», «забивает», «забит»), что позволило находить больше релевантных материалов, связанных с футбольной тематикой.



\subsection*{Запросы с ухудшением качества}

Наряду с улучшениями были выявлены запросы, для которых качество поиска ухудшилось. Основная причина заключается в избыточной агрегации различных слов с одинаковым или близким стемом. В результате в выдачу попадали документы, формально совпадающие по стему, но различающиеся по смыслу. Это характерно, например, для коротких запросов, а также для терминов, у которых стем не является семантически устойчивым.

Дополнительной проблемой является потеря различий между частями речи и значениями слова, которые в исходной форме были различимы, но после стемминга оказались сведены к одному основанию. Это приводит к снижению точности поиска и росту числа ложноположительных совпадений.

Пример ухудшения качества поиска также был выявлен на футбольной тематике. Для некоторых коротких запросов стемминг приводил к избыточному расширению выдачи. Так, для запроса:

\begin{verbatim}
query=матч
before: 66, 104
after:  66, 104, 233, 301
\end{verbatim}

дополнительные документы в выдаче были связаны с косвенными или контекстными упоминаниями (например, анонсы, расписания или обсуждения, не описывающие конкретный матч). Они попали в выдачу из-за совпадения по общему стему, что привело к снижению точности поиска в верхней части результатов.

Дополнительный анализ \texttt{doc\_score} в результатах эксперимента показывает, что ухудшение качества связано не только с расширением множества найденных документов, но и с особенностями ранжирования. Для некоторых документов, формально совпадающих с запросом по стему, значение \texttt{doc\_score} оказывается сопоставимым или превышающим оценку действительно релевантных документов. Это происходит из-за высокой частоты вхождения терма в тексте и отсутствия учёта семантической роли совпадения. В результате такие документы поднимаются в верхнюю часть выдачи, что приводит к снижению точности поиска.


\subsection*{Возможные улучшения}

Для повышения качества поиска без ухудшения остальных запросов возможны следующие улучшения:
\begin{itemize}
    \item введение адаптивного бонуса за точное совпадение, зависящего от длины запроса;
    \item комбинирование стемминга с частичной лемматизацией для наиболее частотных терминов;
    \item использование эвристик для отключения стемминга в однословных запросах;
    \item учёт статистики совместной встречаемости терминов в документе.
\end{itemize}

Таким образом, стемминг в целом положительно влияет на качество поиска, повышая его устойчивость к вариациям словоформ. Однако для достижения оптимального баланса между полнотой и точностью требуется использование гибридных стратегий ранжирования и нормализации.

\pagebreak



\section{Булев индекс}

\subsection*{Внутреннее представление документов после токенизации}
После прошлых ЛР корпус представлен файлом \texttt{tokens.txt}, где каждая строка соответствует одному токену и имеет вид
\texttt{docId<TAB>token}. Таким образом, документ задаётся не «телом текста», а потоком пар \texttt{(docId, token)}. На этапе индексации токены нормализуются по требованию ТЗ: понижается капитализация. В реализации это сделано функцией \texttt{to\_lower\_ascii()}, т.е. гарантированно приводится к нижнему регистру латиница (ASCII).

\subsection*{Алгоритм построения индекса}
Индекс строится без деревьев и хэш-таблиц, поэтому все структуры основаны на \texttt{std::vector} + сортировка + линейные проходы:
\begin{enumerate}
    \item Чтение \texttt{tokens.txt} и накопление массива пар \texttt{pairs}: \texttt{(term, docId)}.
    Одновременно считаются: \texttt{total\_tokens}, \texttt{max\_doc} и суммарная длина токенов \texttt{sum\_term\_len}.
    \item Формирование прямого индекса (forward): из \texttt{ir\_lr2.documents.json} берутся \texttt{url\_norm} (по порядку элементов массива), а заголовок восстанавливается эвристикой из URL.
    \item Сортировка \texttt{pairs} по ключу \texttt{(term, docId)}.
    \item Линейный проход по отсортированному \texttt{pairs}:
    для каждого терма собирается список уникальных \texttt{docId} (повторы отсекаются), вычисляется \texttt{df} и записывается словарь (DICT) + «blob» списков документов (POSTINGS).
    \item Запись бинарного файла \texttt{index.bin} в секционном формате.
\end{enumerate}

\subsection*{Выбранный метод сортировки: достоинства и недостатки}
Используется \texttt{std::sort} (интроспективная сортировка, на практике $O(n\log n)$) по ключу \texttt{(term, docId)}. Плюсы для задачи индексации:
\begin{itemize}
    \item Простая реализация без \texttt{map/unordered\_map}, соответствует ограничениям.
    \item Хорошая локальность данных: после сортировки все вхождения терма лежат подряд, что позволяет строить postings одним линейным проходом.
    \item Легко обеспечить упорядоченность \texttt{docId} внутри postings и удаление повторов.
\end{itemize}
Минусы:
\begin{itemize}
    \item Требуется хранить все пары \texttt{(term, docId)} в памяти: память растёт линейно от числа токенов.
    \item Время доминируется сортировкой: при росте данных в 10--1000 раз именно $n\log n$ станет основной стоимостью.
\end{itemize}
Для больших коллекций типичный путь ускорения --- внешняя сортировка и/или блочная индексация.

\subsection*{Бинарный формат \texttt{index.bin}}
Формат сделан расширяемым: файл состоит из заголовка, таблицы секций и самих секций. Все числа записываются в little-endian.

\paragraph{Header (фиксированная часть, 20 байт):}
\begin{itemize}
    \item \texttt{magic[4]}: ASCII-строка \texttt{"IRIX"} (4 байта)
    \item \texttt{version}: \texttt{u32} (4 байта), текущая версия = 1
    \item \texttt{section\_count}: \texttt{u32} (4 байта)
    \item \texttt{section\_table\_offset}: \texttt{u64} (8 байт) --- смещение таблицы секций от начала файла
\end{itemize}

\paragraph{SectionTable[section\_count] (по 24 байта на запись):}
каждая запись:
\begin{itemize}
    \item \texttt{type}: \texttt{u32} (4 байта) --- тип секции
    \item \texttt{flags}: \texttt{u32} (4 байта) --- флаги/опции (в текущей версии 0)
    \item \texttt{offset}: \texttt{u64} (8 байт) --- смещение секции от начала файла
    \item \texttt{size}: \texttt{u64} (8 байт) --- размер секции в байтах
\end{itemize}

\paragraph{Секции (type):}
\begin{itemize}
    \item \textbf{META} (\texttt{type=4}): простая статистика для отчёта и отладки:
    \begin{itemize}
        \item \texttt{docs\_count}: \texttt{u32}
        \item \texttt{total\_tokens}: \texttt{u64}
        \item \texttt{unique\_terms}: \texttt{u32}
        \item \texttt{avg\_term\_len}: \texttt{f64}
        \item \texttt{build\_ms}: \texttt{f64}
    \end{itemize}

    \item \textbf{DICT} (\texttt{type=1}): словарь термов:
    \begin{itemize}
        \item \texttt{term\_count}: \texttt{u32}
        \item далее \texttt{term\_count} записей:
        \begin{itemize}
            \item \texttt{term\_len}: \texttt{u16}
            \item \texttt{term[term\_len]}: байты UTF-8 (как есть)
            \item \texttt{df}: \texttt{u32} --- document frequency
            \item \texttt{postings\_offset}: \texttt{u64} --- смещение внутри секции POSTINGS
        \end{itemize}
    \end{itemize}

    \item \textbf{POSTINGS} (\texttt{type=2}): поток списков документов:
    \begin{itemize}
        \item для каждого терма хранится \texttt{df} значений \texttt{u32 docId} (отсортированы по возрастанию, без повторов).
        \item начало списка для терма определяется \texttt{postings\_offset} из DICT.
    \end{itemize}

    \item \textbf{FORWARD} (\texttt{type=3}): прямой индекс (для выдачи):
    \begin{itemize}
        \item \texttt{docs\_count}: \texttt{u32}
        \item далее \texttt{docs\_count} записей:
        \begin{itemize}
            \item \texttt{url\_len}: \texttt{u32}, затем \texttt{url[url\_len]}
            \item \texttt{title\_len}: \texttt{u32}, затем \texttt{title[title\_len]}
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{Почему формат расширяемый}
Расширяемость обеспечивается комбинацией:
\begin{itemize}
    \item поля \texttt{version} в заголовке;
    \item таблицы секций: можно добавлять новые секции (например, частоты \texttt{tf}, позиции, skip-указатели, компрессию, кэш заголовков и т.д.), не ломая чтение старых;
    \item поля \texttt{flags} и переменные размеры секций позволяют эволюционировать формат «мягко».
\end{itemize}

\subsection*{Результаты индексации и расчёты}
В ходе эксперимента по построению булева индекса использовался корпус из
34\,583 документов, подготовленный на этапе предыдущей лабораторной работы.
Общее количество токенов в корпусе составляет 25\,738\,456, при среднем числе
1\,259 токенов на документ. Средняя длина одного токена равна 10{,}7 байт, что
соответствует характеристикам корпуса, полученным на этапе токенизации.

Индексация выполняется за линейный проход по потоку токенов с последующей
сортировкой пар \texttt{(term, docId)}. Экспериментально измеренная
производительность реализации составляет порядка
$9{,}7 \cdot 10^{5}$ токенов в секунду. При сохранении данной скорости для
полного корпуса общее время построения индекса оценивается приблизительно
в 26{,}4 секунды. В пересчёте на один документ это составляет около
0{,}76 мс/док, что указывает на хорошую масштабируемость решения при росте
количества документов.

Если рассматривать скорость индексации относительно объёма входных данных,
то поток токенов эквивалентен примерно
$25\,738\,456 \cdot 10{,}7 \approx 2{,}75 \cdot 10^{8}$ байт
($\approx 262{,}6$ МиБ) очищенного текстового материала. Таким образом,
среднее время обработки составляет порядка 0{,}10 мс на 1 КБ текста. Это
показывает, что время работы индексации в текущей реализации определяется
преимущественно вычислительными операциями и сортировкой, а не вводом-выводом.

В целом полученные результаты подтверждают, что реализованный алгоритм
индексации является достаточно эффективным для корпуса данного размера и
подходит в качестве базового решения для булева поиска. При увеличении объёма
данных в 10 и более раз основными ограничивающими факторами станут объём
оперативной памяти (из-за хранения всех пар \texttt{(term, docId)}) и стоимость
сортировки $O(n \log n)$, что делает актуальными такие направления оптимизации,
как блочная индексация, внешняя сортировка и сжатие списков документов.

\paragraph{Средняя длина терма и сравнение со средней длиной токена}
В рамках данной работы средняя длина токена по корпусу составляет 10{,}7 байт и
характеризует токены с учётом их частотности во всех документах. При этом
средняя длина терма определяется как средняя длина строк, соответствующих
уникальным лексическим единицам словаря индекса, без учёта числа их вхождений.

Как правило, средняя длина терма оказывается выше средней длины токена. Это
объясняется тем, что короткие служебные слова (предлоги, союзы, частицы) имеют
высокую частотность и существенно влияют на среднюю длину токена, тогда как при
расчёте средней длины терма все уникальные слова имеют одинаковый вес. В
результате редкие, но более длинные лексемы (имена собственные, составные
наименования, числовые и технические обозначения) оказывают заметное влияние на
среднюю длину терма.

\subsection*{Оптимальность индексации и масштабирование}
Реализованный алгоритм индексации является эффективным базовым решением для
булева поиска и демонстрирует линейную зависимость времени обработки от объёма
входных данных с логарифмической поправкой, обусловленной этапом сортировки.
Основными ограничивающими факторами текущей реализации являются потребление
оперативной памяти и стоимость сортировки массива пар \texttt{(term, docId)}.

При увеличении объёма корпуса в 10 раз объём используемой памяти и время
выполнения возрастут примерно пропорционально, тогда как при росте данных в
100–1000 раз затраты на сортировку $O(n \log n)$ и требования к оперативной
памяти могут стать критичными. Ввод-вывод при текущих объёмах не является
узким местом, однако при дальнейшем масштабировании также начнёт влиять на
общее время построения индекса.

Потенциальные направления оптимизации включают переход к блочной или внешней
индексации, позволяющей ограничить использование оперативной памяти, а также
уменьшение объёма данных за счёт более компактного представления термов и
списков документов. Дополнительный выигрыш может быть получен за счёт
нормализации Unicode и сжатия списков документов, что позволит уменьшить размер
индекса и повысить эффективность последующих операций булева поиска.



\pagebreak


\section{Булев поиск}

В рамках данной работы реализован булев поиск по инвертированному индексу, включающий разбор поисковых запросов, их выполнение и формирование поисковой выдачи. Реализация состоит из двух основных компонентов: 
\begin{itemize}
    \item утилиты командной строки для пакетного выполнения запросов по заранее построенному индексу;
    \item демонстриратора базовой функциональности поиска (форма запроса и выдача результатов).
\end{itemize}

Поиск выполняется над бинарным индексом (\texttt{index.bin}), загружаемым в память один раз при старте приложения, после чего все запросы обрабатываются в оперативной памяти без повторного чтения данных с диска.

\subsection*{Синтаксис и разбор поисковых запросов}

Поддерживаемый синтаксис полностью соответствует техническому заданию:
\begin{itemize}
    \item пробел или оператор \texttt{\&\&} интерпретируются как логическая операция \textbf{И} (AND);
    \item оператор \texttt{||} соответствует логической операции \textbf{ИЛИ} (OR);
    \item оператор \texttt{!} реализует логическую операцию \textbf{НЕТ} (NOT);
    \item допускается использование круглых скобок для явного задания приоритетов.
\end{itemize}

Парсер поисковых запросов реализован с учётом толерантности к пользовательскому вводу: допускается произвольное число пробелов, операторы могут располагаться вплотную к термам или скобкам, а отсутствие явного оператора между соседними термами трактуется как операция AND. Например, запросы
\begin{itemize}
    \item \texttt{футбол матч},
    \item \texttt{футбол\ \&\&\ матч},
    \item \texttt{футбол    матч}
\end{itemize}
обрабатываются эквивалентно.

Разбор запроса выполняется в два этапа:
\begin{enumerate}
    \item лексический анализ, в ходе которого входная строка преобразуется в последовательность токенов (термы, операторы, скобки);
    \item синтаксический анализ с учётом приоритетов операций (\texttt{! > AND > OR}) и скобок, после чего выражение приводится к форме, удобной для вычисления.
\end{enumerate}

\subsection*{Выполнение булевых операций}

Каждому терму сопоставляется posting list — отсортированный по \texttt{docId} список документов, в которых данный терм встречается. Выполнение булевого запроса сводится к последовательному применению операций над этими списками:
\begin{itemize}
    \item операция \textbf{AND} реализуется как пересечение двух отсортированных списков документов;
    \item операция \textbf{OR} — как объединение списков;
    \item операция \textbf{NOT} — как дополнение множества документов терма относительно полного множества документов корпуса.
\end{itemize}

Такой подход обеспечивает линейную сложность операций относительно суммарной длины обрабатываемых posting list’ов и позволяет эффективно выполнять запросы даже при большом размере корпуса.

\subsection*{Утилита командной строки}

Для демонстрации и тестирования реализована утилита командной строки, выполняющая поиск по индексу для каждого запроса из входного потока (по одному запросу на строку). Пример запуска:
\begin{verbatim}
./lr7 index.bin --report search_reports.txt --topres 100 \
    < queries_search.txt > out.tsv
\end{verbatim}

В результате:
\begin{itemize}
    \item файл \texttt{out.tsv} содержит поисковую выдачу (идентификатор документа, заголовок и URL);
    \item файл \texttt{report\_search.txt} содержит информацию о времени выполнения запросов;
    \item в стандартный поток ошибок выводится список самых «медленных» запросов.
\end{itemize}

\subsection*{Производительность поиска}

Скорость выполнения запросов измерялась непосредственно в процессе работы утилиты. Для каждого запроса фиксировалось время выполнения, после чего формировался список наиболее затратных выражений. В ходе эксперимента получены следующие характерные результаты:

\begin{verbatim}
---- TOP 7 slowest queries ----
rank    ms      line    hits    query
1       0.0802  7       3081    (футбол || матч || чемпионат || лига || кубок) && !сборная
2       0.0355  5       1064    футбол && (матч || чемпионат)
3       0.0341  6       1454    (игрок || игроки) && команда
4       0.0318  4       405     футбол && !матч
5       0.0253  2       928     футбол матч
6       0.0232  3       3289    футбол || матч
7       0.0092  1       1333    футбол
--------------------------------
\end{verbatim}

Типичное время выполнения запроса составляет доли миллисекунды, что соответствует десяткам тысяч запросов в секунду при условии, что индекс уже загружен в память. Наиболее быстрыми являются запросы, содержащие один терм без логических операций, тогда как более сложные выражения требуют дополнительного времени.

\subsection*{Причины замедления сложных запросов}

Наибольшее время выполнения наблюдается у запросов, сочетающих несколько факторов:
\begin{itemize}
    \item большое количество операций \texttt{OR} над высокочастотными термами, формирующих крупные промежуточные множества документов;
    \item использование операции \texttt{NOT}, требующей исключения документов из полного множества корпуса;
    \item вложенные скобочные выражения, увеличивающие число промежуточных операций объединения и пересечения.
\end{itemize}

Например, запрос
\[
(\texttt{футбол} \ || \ \texttt{матч} \ || \ \texttt{чемпионат} \ || \ \texttt{лига} \ || \ \texttt{кубок}) \ \&\& \ !\texttt{сборная}
\]
создаёт крупное объединение posting list’ов частотных термов, после чего выполняется операция исключения документов, связанных со словом «сборная», что объясняет его относительно более высокое время выполнения.

\subsection*{Проверка корректности поисковой выдачи}

Корректность реализации булевого поиска проверялась следующими способами:
\begin{enumerate}
    \item сравнением результатов булевых запросов с наивной проверкой наличия термов в документах на ограниченной выборке;
    \item проверкой выполнения законов булевой алгебры (коммутативность, дистрибутивность, законы де Моргана);
    \item тестированием приоритетов операций и влияния скобок на результат;
    \item проверкой устойчивости парсера к различным вариантам пользовательского ввода (лишние пробелы, отсутствие явных операторов).
\end{enumerate}

\subsection*{Результаты эксперимента}

Результаты выполнения поисковых запросов сохраненяюся в файле \texttt{search\_reports.txt}. Этот файл, а также \texttt{out.tsv}, используется в качестве экспериментальных данных при анализе производительности и корректности реализованного булевого поиска.

\pagebreak
